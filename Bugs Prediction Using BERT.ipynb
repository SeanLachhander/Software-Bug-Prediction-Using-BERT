{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.055864,
     "end_time": "2020-12-13T16:45:06.451312",
     "exception": false,
     "start_time": "2020-12-13T16:45:06.395448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GITHUB BUGS PREDICTION\n",
    "(https://www.linkedin.com/in/seanlachhander/) - December 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052055,
     "end_time": "2020-12-13T16:45:06.555689",
     "exception": false,
     "start_time": "2020-12-13T16:45:06.503634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook simplifies the steps to solve github bug predition through a typical workflow using BERT base uncased model. Before we get started with the workflow , the first question that comes into our mind is how do we get started. Well we will get there but let's first understand what exactly the problem statement is and what is expected here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.09005,
     "end_time": "2020-12-13T16:45:06.698639",
     "exception": false,
     "start_time": "2020-12-13T16:45:06.608589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056315,
     "end_time": "2020-12-13T16:45:06.807352",
     "exception": false,
     "start_time": "2020-12-13T16:45:06.751037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* ###  What is BERT?\n",
    "\n",
    "BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\n",
    "\n",
    "* ### What Makes BERT Different?\n",
    "\n",
    "BERT builds upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\n",
    "\n",
    "* ### Why does pretrained language models makes a difference?\n",
    "\n",
    "Pre-trained representations can either be **context-free** or **contextual**, and contextual representations can further be unidirectional or bidirectional. \n",
    "\n",
    "* Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” \n",
    "\n",
    "* Contextual models instead generate a representation of each word that is based on the other words in the sentence. For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” \n",
    "\n",
    "However, BERT represents “bank” using both its previous and next context — “I accessed the ... account” — starting from the very bottom of a deep neural network, making it deeply bidirectional.\n",
    "\n",
    "Read more about Basic BERT [here](http://https://arxiv.org/pdf/1810.04805.pdf).\n",
    "\n",
    "In this Notebook we will be using BERT Base Uncased, from [Hugging Face Library](http://https://huggingface.co/bert-base-uncased). \n",
    "________________________________________\n",
    "**Model: bert-base-uncased**\n",
    "\n",
    "*        12-layer, 768-hidden, 12-heads, 110M parameters.\n",
    "\n",
    "*        Trained on lower-cased English text.\n",
    "\n",
    "```\n",
    "@article{DBLP:journals/corr/abs-1810-04805,\n",
    "  author    = {Jacob Devlin and\n",
    "               Ming{-}Wei Chang and\n",
    "               Kenton Lee and\n",
    "               Kristina Toutanova},\n",
    "  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n",
    "               Understanding},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1810.04805},\n",
    "  year      = {2018},\n",
    "  url       = {http://arxiv.org/abs/1810.04805},\n",
    "  archivePrefix = {arXiv},\n",
    "  eprint    = {1810.04805},\n",
    "  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n",
    "  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05323,
     "end_time": "2020-12-13T16:45:06.912708",
     "exception": false,
     "start_time": "2020-12-13T16:45:06.859478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Problem Statement:**\n",
    "\n",
    " [MachineHack](http://https://www.machinehack.com/) a Biggest Machine Learning Community For Data Science & AI Enthusiastshas comeup with one of the popular Hackathon on GitHub bugs prediction. \n",
    " \n",
    "Detecting defects in software systems is an evergreen topic, since there is no real world software without bugs. There are many algorithms that has come up recently to help developers find bugs in software. The challenges of having curated datasets for own domain specific software , or code relates bug finding is rare. [research paper](http://https://www.researchgate.net/publication/304664263_A_Public_Bug_Database_of_GitHub_Projects_and_Its_Application_in_Bug_Prediction) for gaining more ideas on Bug Prediction Databases and related algorithms that can be used.\n",
    "\n",
    "In this Hackathon proposed by Machinehack , we need to come up with an algorithm that can predict the bugs, features, and questions based on GitHub titles and the text body. \n",
    "\n",
    "\n",
    "The Dataset has three columns Title, Body and Label: \n",
    "\n",
    "*     **-Title** - the title of the GitHub bug, feature question\n",
    "\n",
    "*     **-Body** - the body of the GitHub bug, feature question\n",
    "\n",
    "*     **-Label** - Represents various classes of Labels\n",
    "\n",
    "Once we clean up the dataset [embold_train.json, embold_test.json] and make it ready for the model we need feed the data to a specifc model (Here we using BERT base uncased as pretrained model to predict the expected outcome )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052022,
     "end_time": "2020-12-13T16:45:07.016605",
     "exception": false,
     "start_time": "2020-12-13T16:45:06.964583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "    Bug - 0\n",
    "    Feature - 1\n",
    "    Question - 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053162,
     "end_time": "2020-12-13T16:45:07.122512",
     "exception": false,
     "start_time": "2020-12-13T16:45:07.069350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Workflow\n",
    "\n",
    "1. Problem Statement.\n",
    "1. Geting Started.\n",
    "1. Train and Test data \n",
    "1. Analyze, identify patterns, and explore the data.\n",
    "1. Model, predict and solve the problem.\n",
    "1. Visualize, report, and present the problem solving steps and final solution.\n",
    "1. Supply or submit the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.055314,
     "end_time": "2020-12-13T16:45:07.231468",
     "exception": false,
     "start_time": "2020-12-13T16:45:07.176154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Getting Started**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050678,
     "end_time": "2020-12-13T16:45:07.333552",
     "exception": false,
     "start_time": "2020-12-13T16:45:07.282874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Importing the essential libraries**\n",
    "\n",
    "\n",
    "Importing the required libraries at the begining of your notebook helps you run your code seemless . We frequenly get interrupted by the missing library errors or any other imports which can be avoided by importing them early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:07.447420Z",
     "iopub.status.busy": "2020-12-13T16:45:07.446598Z",
     "iopub.status.idle": "2020-12-13T16:45:16.606924Z",
     "shell.execute_reply": "2020-12-13T16:45:16.607452Z"
    },
    "papermill": {
     "duration": 9.2226,
     "end_time": "2020-12-13T16:45:16.607633",
     "exception": false,
     "start_time": "2020-12-13T16:45:07.385033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056372,
     "end_time": "2020-12-13T16:45:16.722447",
     "exception": false,
     "start_time": "2020-12-13T16:45:16.666075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:16.843010Z",
     "iopub.status.busy": "2020-12-13T16:45:16.842305Z",
     "iopub.status.idle": "2020-12-13T16:45:16.883307Z",
     "shell.execute_reply": "2020-12-13T16:45:16.882631Z"
    },
    "papermill": {
     "duration": 0.105601,
     "end_time": "2020-12-13T16:45:16.883426",
     "exception": false,
     "start_time": "2020-12-13T16:45:16.777825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053424,
     "end_time": "2020-12-13T16:45:16.991244",
     "exception": false,
     "start_time": "2020-12-13T16:45:16.937820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:17.105253Z",
     "iopub.status.busy": "2020-12-13T16:45:17.104384Z",
     "iopub.status.idle": "2020-12-13T16:45:21.212931Z",
     "shell.execute_reply": "2020-12-13T16:45:21.211844Z"
    },
    "papermill": {
     "duration": 4.168747,
     "end_time": "2020-12-13T16:45:21.213076",
     "exception": false,
     "start_time": "2020-12-13T16:45:17.044329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_frame = pd.read_json(\"/Users/lachhans/Downloads/Github_Bug_Dataset/embold_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.075882,
     "end_time": "2020-12-13T16:45:21.366670",
     "exception": false,
     "start_time": "2020-12-13T16:45:21.290788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081687,
     "end_time": "2020-12-13T16:45:21.526227",
     "exception": false,
     "start_time": "2020-12-13T16:45:21.444540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Currently the title and body is in separate fields. We will combine the title and body to keep them into a single field called text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051569,
     "end_time": "2020-12-13T16:45:21.650976",
     "exception": false,
     "start_time": "2020-12-13T16:45:21.599407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:21.768974Z",
     "iopub.status.busy": "2020-12-13T16:45:21.767941Z",
     "iopub.status.idle": "2020-12-13T16:45:23.008890Z",
     "shell.execute_reply": "2020-12-13T16:45:23.009964Z"
    },
    "papermill": {
     "duration": 1.302342,
     "end_time": "2020-12-13T16:45:23.010160",
     "exception": false,
     "start_time": "2020-12-13T16:45:21.707818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \n",
       "0        a y-zoom on the piano roll would be useful.      1  \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0  \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1  \n",
       "3  i think we should stop logging requests to:\\r ...      1  \n",
       "4  expected behavior\\r alarm actions pid on and p...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df= pd.read_json(\"/Users/lachhans/Downloads/Github_Bug_Dataset/embold_train.json\").reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.091824,
     "end_time": "2020-12-13T16:45:23.184458",
     "exception": false,
     "start_time": "2020-12-13T16:45:23.092634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:23.303923Z",
     "iopub.status.busy": "2020-12-13T16:45:23.303264Z",
     "iopub.status.idle": "2020-12-13T16:45:23.800024Z",
     "shell.execute_reply": "2020-12-13T16:45:23.799353Z"
    },
    "papermill": {
     "duration": 0.558267,
     "end_time": "2020-12-13T16:45:23.800140",
     "exception": false,
     "start_time": "2020-12-13T16:45:23.241873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>config question  path-specific environment var...</td>\n",
       "      <td>issue description or question\\r \\r hey @artemg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crash indien vol</td>\n",
       "      <td>de simulator crasht als hij vol zit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unable to mine rocks</td>\n",
       "      <td>sarkasmo starting today, when i hit enter  act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not all whitelists are processed</td>\n",
       "      <td>create following rules... order of creation is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>add ctx menu for idafree 70 and idafree 5</td>\n",
       "      <td>associated with .dll, .dll_, .exe, .exe_, .sc,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  config question  path-specific environment var...   \n",
       "1                                   crash indien vol   \n",
       "2                               unable to mine rocks   \n",
       "3                   not all whitelists are processed   \n",
       "4          add ctx menu for idafree 70 and idafree 5   \n",
       "\n",
       "                                                body  \n",
       "0  issue description or question\\r \\r hey @artemg...  \n",
       "1                de simulator crasht als hij vol zit  \n",
       "2  sarkasmo starting today, when i hit enter  act...  \n",
       "3  create following rules... order of creation is...  \n",
       "4  associated with .dll, .dll_, .exe, .exe_, .sc,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df= pd.read_json(\"/Users/lachhans/Downloads/Github_Bug_Dataset/embold_test.json\").reset_index(drop=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:23.930209Z",
     "iopub.status.busy": "2020-12-13T16:45:23.929401Z",
     "iopub.status.idle": "2020-12-13T16:45:29.304670Z",
     "shell.execute_reply": "2020-12-13T16:45:29.304094Z"
    },
    "papermill": {
     "duration": 5.443203,
     "end_time": "2020-12-13T16:45:29.304824",
     "exception": false,
     "start_time": "2020-12-13T16:45:23.861621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use a 8bit typeface</td>\n",
       "      <td>since this is meant to emulate some old arcade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>implement wireless m-bus binding</td>\n",
       "      <td>_from  chris.pa...@googlemail.com  https://cod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add multilang support for timeago.js</td>\n",
       "      <td>currently it is only  en . \\r required to add ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scaleway - seg-fault on shutdown</td>\n",
       "      <td>tbr  irc  creates a new scaleway instance with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sistema de pintura: no se guardar los nuevos p...</td>\n",
       "      <td>este sp ya estaba asignado a un carro y se enc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                use a 8bit typeface   \n",
       "1                   implement wireless m-bus binding   \n",
       "2               add multilang support for timeago.js   \n",
       "3                   scaleway - seg-fault on shutdown   \n",
       "4  sistema de pintura: no se guardar los nuevos p...   \n",
       "\n",
       "                                                body  label  \n",
       "0  since this is meant to emulate some old arcade...      1  \n",
       "1  _from  chris.pa...@googlemail.com  https://cod...      1  \n",
       "2  currently it is only  en . \\r required to add ...      1  \n",
       "3  tbr  irc  creates a new scaleway instance with...      0  \n",
       "4  este sp ya estaba asignado a un carro y se enc...      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ex_df= pd.read_json(\"/Users/lachhans/Downloads/Github_Bug_Dataset/embold_train_extra.json\").reset_index(drop=True)\n",
    "train_ex_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.055158,
     "end_time": "2020-12-13T16:45:29.415635",
     "exception": false,
     "start_time": "2020-12-13T16:45:29.360477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Looks like both the train and train extra dataframe contains the same content so let's combine them using concatenation command of pandas on index axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.06492,
     "end_time": "2020-12-13T16:45:29.535141",
     "exception": false,
     "start_time": "2020-12-13T16:45:29.470221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Checking the Length of Each Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:29.655844Z",
     "iopub.status.busy": "2020-12-13T16:45:29.655136Z",
     "iopub.status.idle": "2020-12-13T16:45:29.663321Z",
     "shell.execute_reply": "2020-12-13T16:45:29.664031Z"
    },
    "papermill": {
     "duration": 0.070834,
     "end_time": "2020-12-13T16:45:29.664196",
     "exception": false,
     "start_time": "2020-12-13T16:45:29.593362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 150000\n",
      "300000 300000\n",
      "30000 30000\n"
     ]
    }
   ],
   "source": [
    "def dataset_length_check(data_frame):\n",
    "    print(len(data_frame),data_frame.index.shape[-1])\n",
    "                 \n",
    "dataset_length_check(train_df)\n",
    "dataset_length_check(train_ex_df)\n",
    "dataset_length_check(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:29.790645Z",
     "iopub.status.busy": "2020-12-13T16:45:29.789753Z",
     "iopub.status.idle": "2020-12-13T16:45:29.833782Z",
     "shell.execute_reply": "2020-12-13T16:45:29.834394Z"
    },
    "papermill": {
     "duration": 0.110313,
     "end_time": "2020-12-13T16:45:29.834587",
     "exception": false,
     "start_time": "2020-12-13T16:45:29.724274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \n",
       "0        a y-zoom on the piano roll would be useful.      1  \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0  \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1  \n",
       "3  i think we should stop logging requests to:\\r ...      1  \n",
       "4  expected behavior\\r alarm actions pid on and p...      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df,train_ex_df],ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057591,
     "end_time": "2020-12-13T16:45:29.951068",
     "exception": false,
     "start_time": "2020-12-13T16:45:29.893477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If we look at the label field , it is basically divided into three segments as we mentioned earlier:\n",
    "* Bug - 0\n",
    "* Feature - 1\n",
    "* Question - 2\n",
    "\n",
    "Let's categorize the data fields based on their labels as **df_bug** for all the bugs labeled from the dataset. Similarly we do it for features as **df_feature**  and question as **df_question** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:30.194291Z",
     "iopub.status.busy": "2020-12-13T16:45:30.193602Z",
     "iopub.status.idle": "2020-12-13T16:45:30.209970Z",
     "shell.execute_reply": "2020-12-13T16:45:30.209408Z"
    },
    "papermill": {
     "duration": 0.200949,
     "end_time": "2020-12-13T16:45:30.210088",
     "exception": false,
     "start_time": "2020-12-13T16:45:30.009139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 450000 entries, 0 to 449999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   title   450000 non-null  object\n",
      " 1   body    450000 non-null  object\n",
      " 2   label   450000 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:30.333013Z",
     "iopub.status.busy": "2020-12-13T16:45:30.332165Z",
     "iopub.status.idle": "2020-12-13T16:45:30.546655Z",
     "shell.execute_reply": "2020-12-13T16:45:30.547245Z"
    },
    "papermill": {
     "duration": 0.27764,
     "end_time": "2020-12-13T16:45:30.547422",
     "exception": false,
     "start_time": "2020-12-13T16:45:30.269782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Counts of label column: \n",
      " 1    207318\n",
      "0    200481\n",
      "2     42201\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a269c6c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Total Counts of label column: \\n'.format(),train_df['label'].value_counts())\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.countplot(x='label', data=data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:30.677208Z",
     "iopub.status.busy": "2020-12-13T16:45:30.676328Z",
     "iopub.status.idle": "2020-12-13T16:45:30.725851Z",
     "shell.execute_reply": "2020-12-13T16:45:30.725242Z"
    },
    "papermill": {
     "duration": 0.113687,
     "end_time": "2020-12-13T16:45:30.725978",
     "exception": false,
     "start_time": "2020-12-13T16:45:30.612291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bug = train_df[train_df['label']==0]\n",
    "df_feature = train_df[train_df['label']==1]\n",
    "df_question = train_df[train_df['label']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:30.855960Z",
     "iopub.status.busy": "2020-12-13T16:45:30.855104Z",
     "iopub.status.idle": "2020-12-13T16:45:30.867631Z",
     "shell.execute_reply": "2020-12-13T16:45:30.867101Z"
    },
    "papermill": {
     "duration": 0.081318,
     "end_time": "2020-12-13T16:45:30.867759",
     "exception": false,
     "start_time": "2020-12-13T16:45:30.786441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    200481\n",
       "1    207318\n",
       "2     42201\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = train_df.label.value_counts().sort_index()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:30.993191Z",
     "iopub.status.busy": "2020-12-13T16:45:30.992271Z",
     "iopub.status.idle": "2020-12-13T16:45:51.192586Z",
     "shell.execute_reply": "2020-12-13T16:45:51.192001Z"
    },
    "papermill": {
     "duration": 20.264659,
     "end_time": "2020-12-13T16:45:51.192706",
     "exception": false,
     "start_time": "2020-12-13T16:45:30.928047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fx(x):\n",
    "    return x['title'] + \" \" + x['body']   \n",
    "train_df['text']= train_df.apply(lambda x : fx(x),axis=1)\n",
    "test_df['text']= train_df.apply(lambda x : fx(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reindex(columns=[\"title\", \"body\", \"text\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>y-zoom piano roll a y-zoom on the piano roll w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>buggy behavior in selection ! screenshot from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>auto update feature hi,\\r \\r great job so far,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  \\\n",
       "0        a y-zoom on the piano roll would be useful.   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...   \n",
       "3  i think we should stop logging requests to:\\r ...   \n",
       "4  expected behavior\\r alarm actions pid on and p...   \n",
       "\n",
       "                                                text  label  \n",
       "0  y-zoom piano roll a y-zoom on the piano roll w...      1  \n",
       "1  buggy behavior in selection ! screenshot from ...      0  \n",
       "2  auto update feature hi,\\r \\r great job so far,...      1  \n",
       "3  filter out noisy endpoints in logs i think we ...      1  \n",
       "4  enable pid on / pid off alarm actions for ardu...      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(train_df.text, train_df.label, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058262,
     "end_time": "2020-12-13T16:45:51.314866",
     "exception": false,
     "start_time": "2020-12-13T16:45:51.256604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Count the Lables :  Bug, Feature and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-b5fec669aca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:51.443577Z",
     "iopub.status.busy": "2020-12-13T16:45:51.442601Z",
     "iopub.status.idle": "2020-12-13T16:45:51.448090Z",
     "shell.execute_reply": "2020-12-13T16:45:51.447342Z"
    },
    "papermill": {
     "duration": 0.072687,
     "end_time": "2020-12-13T16:45:51.448230",
     "exception": false,
     "start_time": "2020-12-13T16:45:51.375543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints with label as Bug : 200481\n",
      "Number of datapoints with label as Feature : 207318\n",
      "Number of datapoints with label as Question : 42201\n"
     ]
    }
   ],
   "source": [
    "print('Number of datapoints with label as Bug :',label_counts[0])\n",
    "print('Number of datapoints with label as Feature :',label_counts[1])\n",
    "print('Number of datapoints with label as Question :',label_counts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059004,
     "end_time": "2020-12-13T16:45:51.566838",
     "exception": false,
     "start_time": "2020-12-13T16:45:51.507834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's ensure we make text lowercase, remove text in square brackets, remove links, remove punctuation and remove words containing numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080133,
     "end_time": "2020-12-13T16:45:51.705506",
     "exception": false,
     "start_time": "2020-12-13T16:45:51.625373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sanitize the Text Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:51.888637Z",
     "iopub.status.busy": "2020-12-13T16:45:51.887847Z",
     "iopub.status.idle": "2020-12-13T16:45:52.413203Z",
     "shell.execute_reply": "2020-12-13T16:45:52.413788Z"
    },
    "papermill": {
     "duration": 0.613859,
     "end_time": "2020-12-13T16:45:52.413939",
     "exception": false,
     "start_time": "2020-12-13T16:45:51.800080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers'''\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\\\r', '')\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(git_text):\n",
    "    remove_punctuation = [ch for ch in git_text if ch not in punctuation]\n",
    "    # convert them back to sentences and split into words\n",
    "    remove_punctuation = \"\".join(remove_punctuation).split()\n",
    "    filtered_git_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n",
    "    return filtered_git_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "\n",
    "def visulize_dataset(data_frame, category):\n",
    "    \n",
    "    # Let's apply the above two functions 'clean_text' and 'remove_stopwords' to the whole dataset\n",
    "\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: clean_text(x))\n",
    "    data_frame[\"text\"] = data_frame[\"text\"].apply(remove_stopwords)\n",
    "    \n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    for i, j in data_frame.iterrows():\n",
    "        for word in j['text']:\n",
    "            word_list.append(word)\n",
    "        \n",
    "    count_dict = Counter(word_list)\n",
    "    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n",
    "    fig = px.histogram(most_common_words_df,\n",
    "                       x='word', \n",
    "                       y='count',\n",
    "                       title='Most common terms used while refering to a GitHub {}'.format(category),\n",
    "                       color_discrete_sequence=['#843B62'] )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In information retrieval, TF-IDF, short for term frequency–inverse document frequency, \n",
    "is a numerical statistic that is intended to reflect how important a word is to a document \n",
    "in a collection or corpus. We will use TF-IDF to vectorize our text data before feeding them \n",
    "to machine learning algorithms.\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "X_train_preprocessed = np.array([clean_text(text) for text in X_train])\n",
    "X_val_preprocessed = np.array([clean_text(text) for text in X_val])\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
    "X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train Naive Bayes Classifier\n",
    "We will use cross-validation and AUC score to tune hyperparameters of our model. \n",
    "The function get_auc_CV will return the average AUC score from cross-validation.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0652c5a4f60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m res = pd.Series([get_auc_CV(MultinomialNB(i))\n\u001b[0;32m----> 4\u001b[0;31m                  for i in np.arange(1, 10, 0.1)],\n\u001b[0m\u001b[1;32m      5\u001b[0m                 index=np.arange(1, 10, 0.1))\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-0652c5a4f60d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m res = pd.Series([get_auc_CV(MultinomialNB(i))\n\u001b[0;32m----> 4\u001b[0;31m                  for i in np.arange(1, 10, 0.1)],\n\u001b[0m\u001b[1;32m      5\u001b[0m                 index=np.arange(1, 10, 0.1))\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-22ff3734a526>\u001b[0m in \u001b[0;36mget_auc_CV\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Get AUC scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     auc = cross_val_score(\n\u001b[0;32m---> 18\u001b[0;31m         model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 score = scorer._score(cached_call, estimator,\n\u001b[0;32m---> 87\u001b[0;31m                                       *args, **kwargs)\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, clf, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_regressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
    "                 for i in np.arange(1, 10, 0.1)],\n",
    "                index=np.arange(1, 10, 0.1))\n",
    "\n",
    "best_alpha = np.round(res.idxmax(), 2)\n",
    "print('Best alpha: ', best_alpha)\n",
    "\n",
    "plt.plot(res)\n",
    "plt.title('AUC vs. Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ov Validation Set\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.734573368762917,\n",
       "  'recall': 0.8655203260113309,\n",
       "  'f1-score': 0.7946886906527344,\n",
       "  'support': 20122},\n",
       " '1': {'precision': 0.7733784228077591,\n",
       "  'recall': 0.7999417022930432,\n",
       "  'f1-score': 0.7864358208955223,\n",
       "  'support': 20584},\n",
       " '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4294},\n",
       " 'accuracy': 0.7529333333333333,\n",
       " 'macro avg': {'precision': 0.5026505971902254,\n",
       "  'recall': 0.5551540094347914,\n",
       "  'f1-score': 0.5270415038494188,\n",
       "  'support': 45000},\n",
       " 'weighted avg': {'precision': 0.6822290395849407,\n",
       "  'recall': 0.7529333333333333,\n",
       "  'f1-score': 0.7150826837917279,\n",
       "  'support': 45000}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute predicted probabilities\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "nb_model = MultinomialNB(alpha=1.8)\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "probs = nb_model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "#evaluate_roc(probs, y_val)\n",
    "multilabel_confusion_matrix(probs, y_val)\n",
    "classification_report(y_val, probs,output_dict=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": 0.08541,
     "end_time": "2020-12-13T16:45:52.587950",
     "exception": false,
     "start_time": "2020-12-13T16:45:52.502540",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:52.775535Z",
     "iopub.status.busy": "2020-12-13T16:45:52.774599Z",
     "iopub.status.idle": "2020-12-13T16:45:52.777543Z",
     "shell.execute_reply": "2020-12-13T16:45:52.778198Z"
    },
    "papermill": {
     "duration": 0.101102,
     "end_time": "2020-12-13T16:45:52.778372",
     "exception": false,
     "start_time": "2020-12-13T16:45:52.677270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.105295,
     "end_time": "2020-12-13T16:45:53.006351",
     "exception": false,
     "start_time": "2020-12-13T16:45:52.901056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:53.196228Z",
     "iopub.status.busy": "2020-12-13T16:45:53.195303Z",
     "iopub.status.idle": "2020-12-13T16:45:54.535124Z",
     "shell.execute_reply": "2020-12-13T16:45:54.536039Z"
    },
    "papermill": {
     "duration": 1.435682,
     "end_time": "2020-12-13T16:45:54.536265",
     "exception": false,
     "start_time": "2020-12-13T16:45:53.100583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "\n",
    "def visulize_dataset(data_frame, category):\n",
    "    \n",
    "    # Let's apply the above two functions 'clean_text' and 'remove_stopwords' to the whole dataset\n",
    "\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: clean_text(x))\n",
    "    data_frame[\"text\"] = data_frame[\"text\"].apply(remove_stopwords)\n",
    "    \n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    for i, j in data_frame.iterrows():\n",
    "        for word in j['text']:\n",
    "            word_list.append(word)\n",
    "        \n",
    "    count_dict = Counter(word_list)\n",
    "    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n",
    "    fig = px.histogram(most_common_words_df,\n",
    "                       x='word', \n",
    "                       y='count',\n",
    "                       title='Most common terms used while refering to a GitHub {}'.format(category),\n",
    "                       color_discrete_sequence=['#843B62'] )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063457,
     "end_time": "2020-12-13T16:45:54.684646",
     "exception": false,
     "start_time": "2020-12-13T16:45:54.621189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Printing the text field after Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:45:54.812381Z",
     "iopub.status.busy": "2020-12-13T16:45:54.811425Z",
     "iopub.status.idle": "2020-12-13T16:47:32.074504Z",
     "shell.execute_reply": "2020-12-13T16:47:32.075019Z"
    },
    "papermill": {
     "duration": 97.329618,
     "end_time": "2020-12-13T16:47:32.075164",
     "exception": false,
     "start_time": "2020-12-13T16:45:54.745546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>yzoom piano roll a yzoom on the piano roll wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection  screenshot from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi  great job so far saenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on  pid off alarm actions for  expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1  buggy behavior in selection  screenshot from  ...  \n",
       "2  auto update feature hi  great job so far saenz...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on  pid off alarm actions for  expe...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "label 0: Bug\n",
    "label 1: Feature\n",
    "label 2: Question\n",
    "\"\"\"\n",
    "data_frame['text'] = data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "data_frame['text'] = data_frame['text'].apply(lambda x: clean_text(x))\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:47:32.210101Z",
     "iopub.status.busy": "2020-12-13T16:47:32.209329Z",
     "iopub.status.idle": "2020-12-13T16:47:32.214999Z",
     "shell.execute_reply": "2020-12-13T16:47:32.214484Z"
    },
    "papermill": {
     "duration": 0.078143,
     "end_time": "2020-12-13T16:47:32.215104",
     "exception": false,
     "start_time": "2020-12-13T16:47:32.136961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                     title  \\\n",
       "0                                       y-zoom piano roll   \n",
       "1                             buggy behavior in selection   \n",
       "2                                     auto update feature   \n",
       "3                      filter out noisy endpoints in logs   \n",
       "4       enable pid on / pid off alarm actions for ardu...   \n",
       "...                                                   ...   \n",
       "449995                       shield against invalid dates   \n",
       "449996                                     fix the knight   \n",
       "449997  portaudio playback binding crashes often, ubun...   \n",
       "449998  add --db-path option for allowing user to spec...   \n",
       "449999                 staff users cannot log in to admin   \n",
       "\n",
       "                                                     body  label  \\\n",
       "0             a y-zoom on the piano roll would be useful.      1   \n",
       "1       ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2       hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3       i think we should stop logging requests to:\\r ...      1   \n",
       "4       expected behavior\\r alarm actions pid on and p...      0   \n",
       "...                                                   ...    ...   \n",
       "449995  makes'em crash the renderer for now :|\\r \\r da...      0   \n",
       "449996   seriously, if you do just one thing this week...      0   \n",
       "449997         appear to be very unstable. relates to  17      0   \n",
       "449998  add  --db-path  option with default value as  ...      1   \n",
       "449999  i added admire as a staff user but he cannot l...      0   \n",
       "\n",
       "                                                     text  \n",
       "0       yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1       buggy behavior in selection  screenshot from  ...  \n",
       "2       auto update feature hi  great job so far saenz...  \n",
       "3       filter out noisy endpoints in logs i think we ...  \n",
       "4       enable pid on  pid off alarm actions for  expe...  \n",
       "...                                                   ...  \n",
       "449995  shield against invalid dates makesem crash the...  \n",
       "449996  fix the knight seriously if you do just one th...  \n",
       "449997  portaudio playback binding crashes often ubunt...  \n",
       "449998  add dbpath option for allowing user to specify...  \n",
       "449999  staff users cannot log in to admin i added adm...  \n",
       "\n",
       "[450000 rows x 4 columns]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()\n",
    "data_frame.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:47:32.343575Z",
     "iopub.status.busy": "2020-12-13T16:47:32.342801Z",
     "iopub.status.idle": "2020-12-13T16:47:32.351029Z",
     "shell.execute_reply": "2020-12-13T16:47:32.351541Z"
    },
    "papermill": {
     "duration": 0.074605,
     "end_time": "2020-12-13T16:47:32.351662",
     "exception": false,
     "start_time": "2020-12-13T16:47:32.277057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1158fe630>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.06244,
     "end_time": "2020-12-13T16:47:32.476382",
     "exception": false,
     "start_time": "2020-12-13T16:47:32.413942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:47:32.605777Z",
     "iopub.status.busy": "2020-12-13T16:47:32.605000Z",
     "iopub.status.idle": "2020-12-13T16:47:32.657964Z",
     "shell.execute_reply": "2020-12-13T16:47:32.657368Z"
    },
    "papermill": {
     "duration": 0.119468,
     "end_time": "2020-12-13T16:47:32.658086",
     "exception": false,
     "start_time": "2020-12-13T16:47:32.538618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    VALID_BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    BERT_PATH = \"bert-base-uncased\"\n",
    "    MODEL_PATH = \"../input/bert-base-uncased/pytorch_model.bin\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "    truncation=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:47:32.790571Z",
     "iopub.status.busy": "2020-12-13T16:47:32.789675Z",
     "iopub.status.idle": "2020-12-13T16:48:11.389198Z",
     "shell.execute_reply": "2020-12-13T16:48:11.389854Z"
    },
    "papermill": {
     "duration": 38.668971,
     "end_time": "2020-12-13T16:48:11.390039",
     "exception": false,
     "start_time": "2020-12-13T16:47:32.721068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=0.005)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:11.526603Z",
     "iopub.status.busy": "2020-12-13T16:48:11.525890Z",
     "iopub.status.idle": "2020-12-13T16:48:11.530450Z",
     "shell.execute_reply": "2020-12-13T16:48:11.529964Z"
    },
    "papermill": {
     "duration": 0.07362,
     "end_time": "2020-12-13T16:48:11.530550",
     "exception": false,
     "start_time": "2020-12-13T16:48:11.456930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading our BERT model\n",
    "BERT_PATH = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:11.662529Z",
     "iopub.status.busy": "2020-12-13T16:48:11.661530Z",
     "iopub.status.idle": "2020-12-13T16:48:11.703123Z",
     "shell.execute_reply": "2020-12-13T16:48:11.702519Z"
    },
    "papermill": {
     "duration": 0.108963,
     "end_time": "2020-12-13T16:48:11.703231",
     "exception": false,
     "start_time": "2020-12-13T16:48:11.594268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading the pre-trained BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.067601,
     "end_time": "2020-12-13T16:48:11.836016",
     "exception": false,
     "start_time": "2020-12-13T16:48:11.768415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BERT conversion of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:11.985511Z",
     "iopub.status.busy": "2020-12-13T16:48:11.984695Z",
     "iopub.status.idle": "2020-12-13T16:48:11.992327Z",
     "shell.execute_reply": "2020-12-13T16:48:11.991757Z"
    },
    "papermill": {
     "duration": 0.084737,
     "end_time": "2020-12-13T16:48:11.992449",
     "exception": false,
     "start_time": "2020-12-13T16:48:11.907712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: script stopped adding videos saenzramiro abc xyz\n",
      "   Tokens: ['script', 'stopped', 'adding', 'videos', 'sa', '##en', '##z', '##ram', '##iro', 'abc', 'x', '##y', '##z']\n",
      "Token IDs: [5896, 3030, 5815, 6876, 7842, 2368, 2480, 6444, 9711, 5925, 1060, 2100, 2480]\n"
     ]
    }
   ],
   "source": [
    "# some basic operations to understand how BERT converts a sentence into tokens and then into IDs\n",
    "sample_body = 'script stopped adding videos saenzramiro abc xyz'\n",
    "tokens = tokenizer.tokenize(sample_body)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentence: {sample_body}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-bc327e77c2de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-6bad7f8690da>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(git_text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# convert them back to sentences and split into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mremove_punctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfiltered_git_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremove_punctuation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_git_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-6bad7f8690da>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# convert them back to sentences and split into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mremove_punctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfiltered_git_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremove_punctuation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_git_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     23\u001b[0m         return [\n\u001b[1;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpy3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36madd_py3_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PY3_DATA_UPDATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"/PY3\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".zip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: clean_text(x))\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066143,
     "end_time": "2020-12-13T16:48:12.125485",
     "exception": false,
     "start_time": "2020-12-13T16:48:12.059342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Adding [CLS]:101, [SEP]:102, [PAD]:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:12.267290Z",
     "iopub.status.busy": "2020-12-13T16:48:12.266350Z",
     "iopub.status.idle": "2020-12-13T16:48:12.276733Z",
     "shell.execute_reply": "2020-12-13T16:48:12.276086Z"
    },
    "papermill": {
     "duration": 0.085611,
     "end_time": "2020-12-13T16:48:12.276840",
     "exception": false,
     "start_time": "2020-12-13T16:48:12.191229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:12.420627Z",
     "iopub.status.busy": "2020-12-13T16:48:12.419748Z",
     "iopub.status.idle": "2020-12-13T16:48:12.429771Z",
     "shell.execute_reply": "2020-12-13T16:48:12.430269Z"
    },
    "papermill": {
     "duration": 0.085113,
     "end_time": "2020-12-13T16:48:12.430406",
     "exception": false,
     "start_time": "2020-12-13T16:48:12.345293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 5000\n",
    "X = train_df.text.values\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print(X)\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "print('Train tokenizing done...')\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:12.572854Z",
     "iopub.status.busy": "2020-12-13T16:48:12.570993Z",
     "iopub.status.idle": "2020-12-13T16:48:12.573605Z",
     "shell.execute_reply": "2020-12-13T16:48:12.574133Z"
    },
    "papermill": {
     "duration": 0.073757,
     "end_time": "2020-12-13T16:48:12.574260",
     "exception": false,
     "start_time": "2020-12-13T16:48:12.500503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "bert_probs = bert_predict(bert_classifier, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066807,
     "end_time": "2020-12-13T16:48:12.710560",
     "exception": false,
     "start_time": "2020-12-13T16:48:12.643753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implement Class for GitHub Messages - Incl. Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:12.852005Z",
     "iopub.status.busy": "2020-12-13T16:48:12.851028Z",
     "iopub.status.idle": "2020-12-13T16:48:12.862675Z",
     "shell.execute_reply": "2020-12-13T16:48:12.862113Z"
    },
    "papermill": {
     "duration": 0.084704,
     "end_time": "2020-12-13T16:48:12.862788",
     "exception": false,
     "start_time": "2020-12-13T16:48:12.778084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Git_Message(Dataset):\n",
    "    def __init__(self, git_messages, label, tokenizer, max_len):\n",
    "        self.git_messages = git_messages\n",
    "        self.label = label\n",
    "        self.tokenizer = config.tokenizer\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.git_messages)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        git_messages = str(self.git_messages[item])\n",
    "        label = self.label[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        git_messages,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt')\n",
    "        return {\n",
    "        'git_messages': git_messages,\n",
    "         'input_ids': encoding['input_ids'],\n",
    "         'attention_mask': encoding['attention_mask'],\n",
    "         'label': torch.tensor(label, dtype=torch.long)\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.070252,
     "end_time": "2020-12-13T16:48:13.003701",
     "exception": false,
     "start_time": "2020-12-13T16:48:12.933449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating Training Set, Test Set, Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:13.144551Z",
     "iopub.status.busy": "2020-12-13T16:48:13.143640Z",
     "iopub.status.idle": "2020-12-13T16:48:13.158431Z",
     "shell.execute_reply": "2020-12-13T16:48:13.157633Z"
    },
    "papermill": {
     "duration": 0.08811,
     "end_time": "2020-12-13T16:48:13.158789",
     "exception": false,
     "start_time": "2020-12-13T16:48:13.070679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:13.310315Z",
     "iopub.status.busy": "2020-12-13T16:48:13.309335Z",
     "iopub.status.idle": "2020-12-13T16:48:13.313068Z",
     "shell.execute_reply": "2020-12-13T16:48:13.313567Z"
    },
    "papermill": {
     "duration": 0.085082,
     "end_time": "2020-12-13T16:48:13.313687",
     "exception": false,
     "start_time": "2020-12-13T16:48:13.228605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>yzoom piano roll a yzoom on the piano roll wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection  screenshot from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi  great job so far saenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on  pid off alarm actions for  expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1  buggy behavior in selection  screenshot from  ...  \n",
       "2  auto update feature hi  great job so far saenz...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on  pid off alarm actions for  expe...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:13.455254Z",
     "iopub.status.busy": "2020-12-13T16:48:13.454445Z",
     "iopub.status.idle": "2020-12-13T16:48:13.457370Z",
     "shell.execute_reply": "2020-12-13T16:48:13.456849Z"
    },
    "papermill": {
     "duration": 0.075353,
     "end_time": "2020-12-13T16:48:13.457482",
     "exception": false,
     "start_time": "2020-12-13T16:48:13.382129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>yzoom piano roll a yzoom on the piano roll wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection  screenshot from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi  great job so far saenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on  pid off alarm actions for  expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1  buggy behavior in selection  screenshot from  ...  \n",
       "2  auto update feature hi  great job so far saenz...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on  pid off alarm actions for  expe...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:13.602151Z",
     "iopub.status.busy": "2020-12-13T16:48:13.601217Z",
     "iopub.status.idle": "2020-12-13T16:48:13.606492Z",
     "shell.execute_reply": "2020-12-13T16:48:13.607090Z"
    },
    "papermill": {
     "duration": 0.080526,
     "end_time": "2020-12-13T16:48:13.607206",
     "exception": false,
     "start_time": "2020-12-13T16:48:13.526680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(\n",
    "    data_frame,\n",
    "    test_size=0.4,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "testing_data, validation_data = train_test_split(\n",
    "    testing_data,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:13.750377Z",
     "iopub.status.busy": "2020-12-13T16:48:13.749420Z",
     "iopub.status.idle": "2020-12-13T16:48:13.753631Z",
     "shell.execute_reply": "2020-12-13T16:48:13.753123Z"
    },
    "papermill": {
     "duration": 0.078637,
     "end_time": "2020-12-13T16:48:13.753753",
     "exception": false,
     "start_time": "2020-12-13T16:48:13.675116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82204</th>\n",
       "      <td>v-model on md-textarea sets the modeled variab...</td>\n",
       "      <td>\\r \\r &lt;!-- bug report template --&gt;\\r &lt;!-- i in...</td>\n",
       "      <td>0</td>\n",
       "      <td>vmodel on mdtextarea sets the modeled variable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348317</th>\n",
       "      <td>event series: update event card information</td>\n",
       "      <td>update the event card on event series page to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>event series update event card information upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104666</th>\n",
       "      <td>how do i make more than one artifact, using th...</td>\n",
       "      <td>id like to make several artifacts from the sam...</td>\n",
       "      <td>1</td>\n",
       "      <td>how do i make more than one artifact using the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60415</th>\n",
       "      <td>afterrollback not always called when tx tolls ...</td>\n",
       "      <td>when a transaction rolls back due to constrain...</td>\n",
       "      <td>0</td>\n",
       "      <td>afterrollback not always called when tx tolls ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67894</th>\n",
       "      <td>error with typo3 8.6</td>\n",
       "      <td>hi,\\r \\r i'm just testing the new lts. with sf...</td>\n",
       "      <td>1</td>\n",
       "      <td>error with   hi  im just testing the new lts w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435969</th>\n",
       "      <td>show version numbers in about dialogue</td>\n",
       "      <td>show \\r \\r   tool version\\r \\r if cli is start...</td>\n",
       "      <td>1</td>\n",
       "      <td>show version numbers in about dialogue show   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284111</th>\n",
       "      <td>add argument init to multipleruns</td>\n",
       "      <td>add a new argument to  multipleruns  called  i...</td>\n",
       "      <td>1</td>\n",
       "      <td>add argument init to multipleruns add a new ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>configure rules inside of github for audits</td>\n",
       "      <td>allow user to configure custom rules for audit...</td>\n",
       "      <td>1</td>\n",
       "      <td>configure rules inside of github for audits al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>rc releases: gateway fails to start up due to ...</td>\n",
       "      <td>to reproduce:\\r - install enterprise gateway r...</td>\n",
       "      <td>0</td>\n",
       "      <td>rc releases gateway fails to start up due to e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158158</th>\n",
       "      <td>ork boy gunner specialisms</td>\n",
       "      <td>ork boy gunner shouldn't be able to take logis...</td>\n",
       "      <td>0</td>\n",
       "      <td>ork boy gunner specialisms ork boy gunner shou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "82204   v-model on md-textarea sets the modeled variab...   \n",
       "348317        event series: update event card information   \n",
       "104666  how do i make more than one artifact, using th...   \n",
       "60415   afterrollback not always called when tx tolls ...   \n",
       "67894                                error with typo3 8.6   \n",
       "435969             show version numbers in about dialogue   \n",
       "284111                  add argument init to multipleruns   \n",
       "6098          configure rules inside of github for audits   \n",
       "4664    rc releases: gateway fails to start up due to ...   \n",
       "158158                         ork boy gunner specialisms   \n",
       "\n",
       "                                                     body  label  \\\n",
       "82204   \\r \\r <!-- bug report template -->\\r <!-- i in...      0   \n",
       "348317  update the event card on event series page to ...      0   \n",
       "104666  id like to make several artifacts from the sam...      1   \n",
       "60415   when a transaction rolls back due to constrain...      0   \n",
       "67894   hi,\\r \\r i'm just testing the new lts. with sf...      1   \n",
       "435969  show \\r \\r   tool version\\r \\r if cli is start...      1   \n",
       "284111  add a new argument to  multipleruns  called  i...      1   \n",
       "6098    allow user to configure custom rules for audit...      1   \n",
       "4664    to reproduce:\\r - install enterprise gateway r...      0   \n",
       "158158  ork boy gunner shouldn't be able to take logis...      0   \n",
       "\n",
       "                                                     text  \n",
       "82204   vmodel on mdtextarea sets the modeled variable...  \n",
       "348317  event series update event card information upd...  \n",
       "104666  how do i make more than one artifact using the...  \n",
       "60415   afterrollback not always called when tx tolls ...  \n",
       "67894   error with   hi  im just testing the new lts w...  \n",
       "435969  show version numbers in about dialogue show   ...  \n",
       "284111  add argument init to multipleruns add a new ar...  \n",
       "6098    configure rules inside of github for audits al...  \n",
       "4664    rc releases gateway fails to start up due to e...  \n",
       "158158  ork boy gunner specialisms ork boy gunner shou...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape, testing_data.shape, validation_data.shape\n",
    "validation_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.073187,
     "end_time": "2020-12-13T16:48:13.897781",
     "exception": false,
     "start_time": "2020-12-13T16:48:13.824594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:14.048489Z",
     "iopub.status.busy": "2020-12-13T16:48:14.047550Z",
     "iopub.status.idle": "2020-12-13T16:48:14.050567Z",
     "shell.execute_reply": "2020-12-13T16:48:14.050026Z"
    },
    "papermill": {
     "duration": 0.082842,
     "end_time": "2020-12-13T16:48:14.050676",
     "exception": false,
     "start_time": "2020-12-13T16:48:13.967834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data_loader(data, tokenizer, max_len, batch_size):\n",
    "    \n",
    "    ds = Git_Message(git_messages=data.text.to_numpy(),\n",
    "    label=data.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len)\n",
    "    \n",
    "    return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_data_loader = create_data_loader(training_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "testing_data_loader = create_data_loader(testing_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(validation_data, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:14.194329Z",
     "iopub.status.busy": "2020-12-13T16:48:14.193357Z",
     "iopub.status.idle": "2020-12-13T16:48:14.756021Z",
     "shell.execute_reply": "2020-12-13T16:48:14.755495Z"
    },
    "papermill": {
     "duration": 0.636209,
     "end_time": "2020-12-13T16:48:14.756135",
     "exception": false,
     "start_time": "2020-12-13T16:48:14.119926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['git_messages', 'input_ids', 'attention_mask', 'label'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = next(iter(train_data_loader))\n",
    "data_frame.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:14.912124Z",
     "iopub.status.busy": "2020-12-13T16:48:14.911325Z",
     "iopub.status.idle": "2020-12-13T16:48:14.916519Z",
     "shell.execute_reply": "2020-12-13T16:48:14.917046Z"
    },
    "papermill": {
     "duration": 0.087903,
     "end_time": "2020-12-13T16:48:14.917169",
     "exception": false,
     "start_time": "2020-12-13T16:48:14.829266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 128]), torch.Size([16, 128]), torch.Size([16]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['input_ids'].squeeze().shape, data_frame['attention_mask'].squeeze().shape, data_frame['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:15.066424Z",
     "iopub.status.busy": "2020-12-13T16:48:15.065604Z",
     "iopub.status.idle": "2020-12-13T16:48:15.077504Z",
     "shell.execute_reply": "2020-12-13T16:48:15.078411Z"
    },
    "papermill": {
     "duration": 0.089743,
     "end_time": "2020-12-13T16:48:15.078608",
     "exception": false,
     "start_time": "2020-12-13T16:48:14.988865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git_messages  :  vignette is not installed i noticed that the  gitignore  file includes the directory  instdoc  where normally the vignette should be located in  html  rmd and  pdf format hence the command  vignette  basic    fails after installing the package from github because there is no vignette available  i wonder if this is done by intention if not please remove the gitignore entry \n",
      "input_ids :  tensor([  101,  6819, 10177,  4674,  2003,  2025,  5361,  1045,  4384,  2008,\n",
      "         1996, 21025,  3775, 26745,  2890,  5371,  2950,  1996, 14176, 16021,\n",
      "         2102,  3527,  2278,  2073,  5373,  1996,  6819, 10177,  4674,  2323,\n",
      "         2022,  2284,  1999, 16129, 28549,  2094,  1998, 11135,  4289,  6516,\n",
      "         1996,  3094,  6819, 10177,  4674,  3937, 11896,  2044, 23658,  1996,\n",
      "         7427,  2013, 21025,  2705, 12083,  2138,  2045,  2003,  2053,  6819,\n",
      "        10177,  4674,  2800,  1045,  4687,  2065,  2023,  2003,  2589,  2011,\n",
      "         6808,  2065,  2025,  3531,  6366,  1996, 21025,  3775, 26745,  2890,\n",
      "         4443,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "attention_mask :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "label :  tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print('git_messages  : ', data_frame['git_messages'][0])\n",
    "print('input_ids : ', data_frame['input_ids'].squeeze()[0])\n",
    "print('attention_mask : ', data_frame['attention_mask'].squeeze()[0])\n",
    "print('label : ', data_frame['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:15.227407Z",
     "iopub.status.busy": "2020-12-13T16:48:15.226812Z",
     "iopub.status.idle": "2020-12-13T16:48:23.698914Z",
     "shell.execute_reply": "2020-12-13T16:48:23.697503Z"
    },
    "papermill": {
     "duration": 8.548246,
     "end_time": "2020-12-13T16:48:23.699032",
     "exception": false,
     "start_time": "2020-12-13T16:48:15.150786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:23.847701Z",
     "iopub.status.busy": "2020-12-13T16:48:23.846811Z",
     "iopub.status.idle": "2020-12-13T16:48:24.044973Z",
     "shell.execute_reply": "2020-12-13T16:48:24.044331Z"
    },
    "papermill": {
     "duration": 0.27487,
     "end_time": "2020-12-13T16:48:24.045096",
     "exception": false,
     "start_time": "2020-12-13T16:48:23.770226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "  input_ids=encodings['input_ids'],\n",
    "  attention_mask=encodings['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:24.198497Z",
     "iopub.status.busy": "2020-12-13T16:48:24.197572Z",
     "iopub.status.idle": "2020-12-13T16:48:24.201521Z",
     "shell.execute_reply": "2020-12-13T16:48:24.202034Z"
    },
    "papermill": {
     "duration": 0.083097,
     "end_time": "2020-12-13T16:48:24.202172",
     "exception": false,
     "start_time": "2020-12-13T16:48:24.119075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape, pooled_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.072693,
     "end_time": "2020-12-13T16:48:24.350214",
     "exception": false,
     "start_time": "2020-12-13T16:48:24.277521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Predictor Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:24.505248Z",
     "iopub.status.busy": "2020-12-13T16:48:24.503343Z",
     "iopub.status.idle": "2020-12-13T16:48:24.506022Z",
     "shell.execute_reply": "2020-12-13T16:48:24.506526Z"
    },
    "papermill": {
     "duration": 0.084541,
     "end_time": "2020-12-13T16:48:24.506647",
     "exception": false,
     "start_time": "2020-12-13T16:48:24.422106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BugPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(BugPredictor, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.out = nn.Linear(self.bert_model.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask = attention_mask\n",
    "        )\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071553,
     "end_time": "2020-12-13T16:48:24.649391",
     "exception": false,
     "start_time": "2020-12-13T16:48:24.577838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Let's Start Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:24.800179Z",
     "iopub.status.busy": "2020-12-13T16:48:24.799433Z",
     "iopub.status.idle": "2020-12-13T16:48:27.528432Z",
     "shell.execute_reply": "2020-12-13T16:48:27.527289Z"
    },
    "papermill": {
     "duration": 2.806113,
     "end_time": "2020-12-13T16:48:27.528553",
     "exception": false,
     "start_time": "2020-12-13T16:48:24.722440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "label 0: Bug\n",
    "label 1: Feature\n",
    "label 2: Question\n",
    "\"\"\"\n",
    "class_names = [0, 1, 2]\n",
    "bug_predictor_model = BugPredictor(len(class_names))\n",
    "bug_predictor_model = bug_predictor_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:27.684874Z",
     "iopub.status.busy": "2020-12-13T16:48:27.684120Z",
     "iopub.status.idle": "2020-12-13T16:48:27.687388Z",
     "shell.execute_reply": "2020-12-13T16:48:27.687879Z"
    },
    "papermill": {
     "duration": 0.086499,
     "end_time": "2020-12-13T16:48:27.688005",
     "exception": false,
     "start_time": "2020-12-13T16:48:27.601506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(bug_predictor_model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:27.849899Z",
     "iopub.status.busy": "2020-12-13T16:48:27.848133Z",
     "iopub.status.idle": "2020-12-13T16:48:27.851245Z",
     "shell.execute_reply": "2020-12-13T16:48:27.851825Z"
    },
    "papermill": {
     "duration": 0.091786,
     "end_time": "2020-12-13T16:48:27.851958",
     "exception": false,
     "start_time": "2020-12-13T16:48:27.760172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].squeeze().to(device)\n",
    "        attention_mask = d['attention_mask'].squeeze().to(device)\n",
    "        targets = d['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        correct_predictions += torch.sum(preds==targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:28.013819Z",
     "iopub.status.busy": "2020-12-13T16:48:28.012895Z",
     "iopub.status.idle": "2020-12-13T16:48:28.015644Z",
     "shell.execute_reply": "2020-12-13T16:48:28.015128Z"
    },
    "papermill": {
     "duration": 0.087874,
     "end_time": "2020-12-13T16:48:28.015775",
     "exception": false,
     "start_time": "2020-12-13T16:48:27.927901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d['input_ids'].squeeze().to(device)\n",
    "            attention_mask = d['attention_mask'].squeeze().to(device)\n",
    "            targets = d['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds==targets)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:48:28.170392Z",
     "iopub.status.busy": "2020-12-13T16:48:28.169461Z",
     "iopub.status.idle": "2020-12-13T16:53:19.031341Z",
     "shell.execute_reply": "2020-12-13T16:53:19.030683Z"
    },
    "papermill": {
     "duration": 290.943388,
     "end_time": "2020-12-13T16:53:19.031468",
     "exception": false,
     "start_time": "2020-12-13T16:48:28.088080",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}/{}'.format(epoch+1,EPOCHS))\n",
    "    print('-' * 10)\n",
    "  \n",
    "    train_acc, train_loss = train_model(bug_predictor_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(training_data))\n",
    "    \n",
    "    print('Train loss : {} accuracy : {}'.format(train_loss, train_acc))\n",
    "    \n",
    "    val_acc, val_loss = eval_model(bug_predictor_model, val_data_loader, loss_fn, device, len(validation_data))\n",
    "    \n",
    "    print('Validation loss : {} accuracy : {}'.format(val_loss, val_acc))\n",
    "    \n",
    "    test_acc, test_loss = eval_model(bug_predictor_model, testing_data_loader, loss_fn, device, len(testing_data))\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        print('Saving the best model ...')\n",
    "        torch.save(bug_predictor_model.state_dict(), 'best_model.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.118948,
     "end_time": "2020-12-13T16:53:19.268292",
     "exception": false,
     "start_time": "2020-12-13T16:53:19.149344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:53:19.510231Z",
     "iopub.status.busy": "2020-12-13T16:53:19.508258Z",
     "iopub.status.idle": "2020-12-13T16:53:19.511071Z",
     "shell.execute_reply": "2020-12-13T16:53:19.511559Z"
    },
    "papermill": {
     "duration": 0.130726,
     "end_time": "2020-12-13T16:53:19.511699",
     "exception": false,
     "start_time": "2020-12-13T16:53:19.380973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_bug_message = \"Script stopped adding video's. A recent change in the youtube layout broke the script. Probably caused by element names being altered.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:53:19.744486Z",
     "iopub.status.busy": "2020-12-13T16:53:19.743667Z",
     "iopub.status.idle": "2020-12-13T16:53:19.748016Z",
     "shell.execute_reply": "2020-12-13T16:53:19.747376Z"
    },
    "papermill": {
     "duration": 0.123831,
     "end_time": "2020-12-13T16:53:19.748126",
     "exception": false,
     "start_time": "2020-12-13T16:53:19.624295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = ['bug', 'feature', 'question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:53:20.106910Z",
     "iopub.status.busy": "2020-12-13T16:53:20.104634Z",
     "iopub.status.idle": "2020-12-13T16:53:20.107988Z",
     "shell.execute_reply": "2020-12-13T16:53:20.108766Z"
    },
    "papermill": {
     "duration": 0.192341,
     "end_time": "2020-12-13T16:53:20.108949",
     "exception": false,
     "start_time": "2020-12-13T16:53:19.916608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_git_category(sample_message, model):\n",
    "    encoded_message = tokenizer.encode_plus(sample_bug_message, max_length=MAX_LENGTH, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids = encoded_message['input_ids'].to(device)\n",
    "    attention_mask = encoded_message['attention_mask'].to(device)\n",
    "    \n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    _, prediction_idx = torch.max(output, dim=1)\n",
    "        \n",
    "    return class_names[prediction_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T16:53:20.455798Z",
     "iopub.status.busy": "2020-12-13T16:53:20.454853Z",
     "iopub.status.idle": "2020-12-13T16:53:20.490924Z",
     "shell.execute_reply": "2020-12-13T16:53:20.490205Z"
    },
    "papermill": {
     "duration": 0.214127,
     "end_time": "2020-12-13T16:53:20.491081",
     "exception": false,
     "start_time": "2020-12-13T16:53:20.276954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Sample bug message : ', sample_bug_message)\n",
    "print('Predicted GitHub Category : ', predict_git_category(sample_bug_message, bug_predictor_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.168235,
     "end_time": "2020-12-13T16:53:20.829450",
     "exception": false,
     "start_time": "2020-12-13T16:53:20.661215",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.153961,
     "end_time": "2020-12-13T16:53:21.156863",
     "exception": false,
     "start_time": "2020-12-13T16:53:21.002902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.plot(history['test_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Testing'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_acc'])\n",
    "plt.plot(history['val_acc'])\n",
    "plt.plot(history['test_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Testing'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 499.974064,
   "end_time": "2020-12-13T16:53:21.989640",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-13T16:45:02.015576",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a18c659b7ca4ba68bcc0b8d01067530": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a754c04f07a46cb94f18be34677cefd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2cd9f19500cf4b67af0ca9f5c1494ca5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_554fc2f94f214242b1a49ee2e0c18d5e",
       "placeholder": "​",
       "style": "IPY_MODEL_ae7d30b25b71472f8fdbea49e868d642",
       "value": " 440M/440M [00:29&lt;00:00, 15.1MB/s]"
      }
     },
     "36e41377ce9e403d82f1fa285bec07ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0a18c659b7ca4ba68bcc0b8d01067530",
       "max": 440473133,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b150f667c2db41c3b1e6b5a3cef0df9d",
       "value": 440473133
      }
     },
     "554fc2f94f214242b1a49ee2e0c18d5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5673453c7b7b43ce80904849cf35da02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5ee1524fb4c2482e9bb36496385f6861": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "69201bc6f446494cb71752406bc2e2bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "8a0550c7fe314a59b39e5152da44fd31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8aea9437bd87477a9155de5bb2734855": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5ee1524fb4c2482e9bb36496385f6861",
       "placeholder": "​",
       "style": "IPY_MODEL_8a0550c7fe314a59b39e5152da44fd31",
       "value": " 433/433 [00:00&lt;00:00, 5.68kB/s]"
      }
     },
     "ae7d30b25b71472f8fdbea49e868d642": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b150f667c2db41c3b1e6b5a3cef0df9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "ce6847e4e60d443bbb755316fd7efa72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eb7af668da474af7b9a1f6b00040bb75",
        "IPY_MODEL_8aea9437bd87477a9155de5bb2734855"
       ],
       "layout": "IPY_MODEL_dc2afa561c364b049d68637559fa0da2"
      }
     },
     "dc2afa561c364b049d68637559fa0da2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb7af668da474af7b9a1f6b00040bb75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1a754c04f07a46cb94f18be34677cefd",
       "max": 433,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_69201bc6f446494cb71752406bc2e2bd",
       "value": 433
      }
     },
     "f9e7726d0af64c46afee749c8b22d6ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_36e41377ce9e403d82f1fa285bec07ea",
        "IPY_MODEL_2cd9f19500cf4b67af0ca9f5c1494ca5"
       ],
       "layout": "IPY_MODEL_5673453c7b7b43ce80904849cf35da02"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
